"""
Model training utilities and pipeline for sign language recognition
Handles training, validation, and model evaluation
"""
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
import json
import logging
from pathlib import Path
import time
from typing import Dict, List, Tuple, Optional
import pickle

from config.config import config

class ModelTrainingPipeline:
    """Complete pipeline for training gesture recognition models"""
    
    def __init__(self, model_factory, data_preprocessor):
        self.model_factory = model_factory
        self.data_preprocessor = data_preprocessor
        self.training_history = {}
        self.evaluation_results = {}
        
        # Setup logging
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)
    
    def prepare_training_data(
        self, 
        data_path: str, 
        validation_split: float = None
    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        """
        Prepare training and validation data
        
        Args:
            data_path: Path to processed data
            validation_split: Fraction of data for validation
            
        Returns:
            X_train, X_val, y_train, y_val
        """
        if validation_split is None:
            validation_split = config.model.validation_split
        
        self.logger.info(f\"Loading data from {data_path}\")\n        \n        # Load processed data\n        data_path = Path(data_path)\n        \n        try:\n            frames = np.load(data_path / 'frames.npy', allow_pickle=True)\n            landmarks = np.load(data_path / 'landmarks.npy', allow_pickle=True)\n            labels = np.load(data_path / 'labels.npy', allow_pickle=True)\n            \n            with open(data_path / 'metadata.json', 'r') as f:\n                metadata = json.load(f)\n            \n            self.logger.info(f\"Loaded {len(frames)} samples with {len(metadata['classes'])} classes\")\n            \n        except FileNotFoundError as e:\n            self.logger.error(f\"Data files not found: {e}\")\n            # Create synthetic data for demonstration\n            return self._create_synthetic_data(validation_split)\n        \n        # Prepare features and labels\n        X = self._prepare_features(frames, landmarks)\n        y = self._prepare_labels(labels, metadata['classes'])\n        \n        # Split data\n        X_train, X_val, y_train, y_val = train_test_split(\n            X, y, test_size=validation_split, random_state=42, stratify=y.argmax(axis=1)\n        )\n        \n        self.logger.info(f\"Training samples: {len(X_train)}, Validation samples: {len(X_val)}\")\n        \n        return X_train, X_val, y_train, y_val\n    \n    def _prepare_features(self, frames: np.ndarray, landmarks: np.ndarray) -> np.ndarray:\n        \"\"\"Prepare feature data based on model type\"\"\"\n        if config.model.model_type == \"cnn\":\n            # Use single frames for CNN\n            return frames[:, 0]  # Take first frame of each sequence\n        elif config.model.model_type == \"cnn_lstm\":\n            # Use full frame sequences\n            return frames\n        elif config.model.model_type == \"transformer\":\n            # Use landmark sequences\n            return self._process_landmarks(landmarks)\n        else:\n            return frames\n    \n    def _process_landmarks(self, landmarks: np.ndarray) -> np.ndarray:\n        \"\"\"Process landmarks for transformer model\"\"\"\n        processed_landmarks = []\n        \n        for sequence in landmarks:\n            sequence_features = []\n            for frame_landmarks in sequence:\n                # Combine hand and pose landmarks\n                features = []\n                \n                # Hand landmarks\n                if frame_landmarks['hands']:\n                    for hand in frame_landmarks['hands']:\n                        features.extend(hand.flatten())\n                else:\n                    features.extend([0] * 63)  # 21 landmarks * 3 coordinates\n                \n                # Pose landmarks\n                if len(frame_landmarks['pose']) > 0:\n                    features.extend(frame_landmarks['pose'])\n                else:\n                    features.extend([0] * 51)  # 17 landmarks * 3 coordinates\n                \n                sequence_features.append(features)\n            \n            processed_landmarks.append(sequence_features)\n        \n        return np.array(processed_landmarks)\n    \n    def _prepare_labels(self, labels: np.ndarray, classes: List[str]) -> np.ndarray:\n        \"\"\"Convert labels to one-hot encoding\"\"\"\n        from sklearn.preprocessing import LabelEncoder\n        \n        # Create label encoder\n        label_encoder = LabelEncoder()\n        label_encoder.fit(classes)\n        \n        # Encode labels\n        encoded_labels = label_encoder.transform(labels)\n        \n        # Convert to one-hot\n        num_classes = len(classes)\n        one_hot_labels = np.eye(num_classes)[encoded_labels]\n        \n        # Save label encoder for later use\n        with open(config.model.model_save_path + 'label_encoder.pkl', 'wb') as f:\n            pickle.dump(label_encoder, f)\n        \n        return one_hot_labels\n    \n    def _create_synthetic_data(self, validation_split: float) -> Tuple:\n        \"\"\"Create synthetic data for testing purposes\"\"\"\n        self.logger.warning(\"Creating synthetic data for demonstration\")\n        \n        # Generate synthetic gesture data\n        num_samples = 1000\n        num_classes = len(config.data.asl_classes)\n        \n        if config.model.model_type == \"cnn\":\n            X = np.random.randn(num_samples, *config.model.input_shape)\n        elif config.model.model_type == \"cnn_lstm\":\n            X = np.random.randn(num_samples, config.data.sequence_length, *config.model.input_shape)\n        else:\n            X = np.random.randn(num_samples, config.data.sequence_length, 114)  # Combined features\n        \n        # Generate random labels\n        y = np.random.randint(0, num_classes, num_samples)\n        y = np.eye(num_classes)[y]\n        \n        # Split data\n        split_idx = int(num_samples * (1 - validation_split))\n        X_train, X_val = X[:split_idx], X[split_idx:]\n        y_train, y_val = y[:split_idx], y[split_idx:]\n        \n        return X_train, X_val, y_train, y_val\n    \n    def train_model(\n        self,\n        X_train: np.ndarray,\n        X_val: np.ndarray,\n        y_train: np.ndarray,\n        y_val: np.ndarray,\n        model_name: str = \"gesture_model\"\n    ) -> Dict:\n        \"\"\"Train the gesture recognition model\"\"\"\n        self.logger.info(f\"Starting training for {config.model.model_type} model\")\n        \n        # Create model\n        num_classes = y_train.shape[1]\n        if config.model.model_type == \"cnn\":\n            input_shape = X_train.shape[1:]\n        else:\n            input_shape = X_train.shape[1:]\n        \n        model = self.model_factory.create_model(\n            config.model.model_type,\n            input_shape,\n            num_classes\n        )\n        \n        # Compile model\n        model.compile(\n            optimizer='adam',\n            loss='categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        # Training callbacks\n        callbacks = self._get_training_callbacks(model_name)\n        \n        # Train model\n        start_time = time.time()\n        history = model.fit(\n            X_train, y_train,\n            validation_data=(X_val, y_val),\n            epochs=config.model.epochs,\n            batch_size=config.model.batch_size,\n            callbacks=callbacks,\n            verbose=1\n        )\n        \n        training_time = time.time() - start_time\n        self.logger.info(f\"Training completed in {training_time:.2f} seconds\")\n        \n        # Save training history\n        self.training_history[model_name] = {\n            'history': history.history,\n            'training_time': training_time,\n            'final_accuracy': history.history['accuracy'][-1],\n            'final_val_accuracy': history.history['val_accuracy'][-1]\n        }\n        \n        # Save model\n        model_path = config.model.model_save_path + f\"{model_name}.h5\"\n        model.save(model_path)\n        self.logger.info(f\"Model saved to {model_path}\")\n        \n        return {\n            'model': model,\n            'history': history,\n            'training_time': training_time\n        }\n    \n    def _get_training_callbacks(self, model_name: str) -> List:\n        \"\"\"Get training callbacks\"\"\"\n        from tensorflow.keras.callbacks import (\n            EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, CSVLogger\n        )\n        \n        callbacks = [\n            EarlyStopping(\n                monitor='val_accuracy',\n                patience=config.model.early_stopping_patience,\n                restore_best_weights=True\n            ),\n            ReduceLROnPlateau(\n                monitor='val_loss',\n                factor=0.5,\n                patience=5,\n                min_lr=1e-7\n            ),\n            ModelCheckpoint(\n                filepath=config.model.checkpoint_path + f'{model_name}_best.h5',\n                monitor='val_accuracy',\n                save_best_only=True\n            ),\n            CSVLogger(\n                config.model.model_save_path + f'{model_name}_training.log'\n            )\n        ]\n        \n        return callbacks\n    \n    def evaluate_model(\n        self,\n        model,\n        X_test: np.ndarray,\n        y_test: np.ndarray,\n        model_name: str = \"gesture_model\"\n    ) -> Dict:\n        \"\"\"Evaluate model performance\"\"\"\n        self.logger.info(f\"Evaluating model {model_name}\")\n        \n        # Predictions\n        y_pred = model.predict(X_test)\n        y_pred_classes = np.argmax(y_pred, axis=1)\n        y_true_classes = np.argmax(y_test, axis=1)\n        \n        # Calculate metrics\n        accuracy = accuracy_score(y_true_classes, y_pred_classes)\n        \n        # Classification report\n        try:\n            with open(config.model.model_save_path + 'label_encoder.pkl', 'rb') as f:\n                label_encoder = pickle.load(f)\n            class_names = label_encoder.classes_\n        except:\n            class_names = [f\"Class_{i}\" for i in range(len(np.unique(y_true_classes)))]\n        \n        report = classification_report(\n            y_true_classes, y_pred_classes,\n            target_names=class_names,\n            output_dict=True\n        )\n        \n        # Confusion matrix\n        cm = confusion_matrix(y_true_classes, y_pred_classes)\n        \n        # Store results\n        self.evaluation_results[model_name] = {\n            'accuracy': accuracy,\n            'classification_report': report,\n            'confusion_matrix': cm,\n            'class_names': class_names\n        }\n        \n        self.logger.info(f\"Model accuracy: {accuracy:.4f}\")\n        \n        return self.evaluation_results[model_name]\n    \n    def plot_training_history(self, model_name: str, save_path: str = None):\n        \"\"\"Plot training history\"\"\"\n        if model_name not in self.training_history:\n            self.logger.error(f\"No training history found for {model_name}\")\n            return\n        \n        history = self.training_history[model_name]['history']\n        \n        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n        \n        # Accuracy\n        axes[0, 0].plot(history['accuracy'], label='Training Accuracy')\n        axes[0, 0].plot(history['val_accuracy'], label='Validation Accuracy')\n        axes[0, 0].set_title('Model Accuracy')\n        axes[0, 0].set_xlabel('Epoch')\n        axes[0, 0].set_ylabel('Accuracy')\n        axes[0, 0].legend()\n        \n        # Loss\n        axes[0, 1].plot(history['loss'], label='Training Loss')\n        axes[0, 1].plot(history['val_loss'], label='Validation Loss')\n        axes[0, 1].set_title('Model Loss')\n        axes[0, 1].set_xlabel('Epoch')\n        axes[0, 1].set_ylabel('Loss')\n        axes[0, 1].legend()\n        \n        # Learning rate (if available)\n        if 'lr' in history:\n            axes[1, 0].plot(history['lr'])\n            axes[1, 0].set_title('Learning Rate')\n            axes[1, 0].set_xlabel('Epoch')\n            axes[1, 0].set_ylabel('Learning Rate')\n        \n        # Training summary\n        summary_text = f\"\"\"\n        Final Training Accuracy: {self.training_history[model_name]['final_accuracy']:.4f}\n        Final Validation Accuracy: {self.training_history[model_name]['final_val_accuracy']:.4f}\n        Training Time: {self.training_history[model_name]['training_time']:.2f}s\n        \"\"\"\n        axes[1, 1].text(0.1, 0.5, summary_text, fontsize=12, verticalalignment='center')\n        axes[1, 1].axis('off')\n        \n        plt.tight_layout()\n        \n        if save_path:\n            plt.savefig(save_path)\n            self.logger.info(f\"Training history plot saved to {save_path}\")\n        \n        plt.show()\n    \n    def plot_confusion_matrix(self, model_name: str, save_path: str = None):\n        \"\"\"Plot confusion matrix\"\"\"\n        if model_name not in self.evaluation_results:\n            self.logger.error(f\"No evaluation results found for {model_name}\")\n            return\n        \n        results = self.evaluation_results[model_name]\n        cm = results['confusion_matrix']\n        class_names = results['class_names']\n        \n        plt.figure(figsize=(12, 10))\n        sns.heatmap(\n            cm, \n            annot=True, \n            fmt='d', \n            cmap='Blues',\n            xticklabels=class_names,\n            yticklabels=class_names\n        )\n        plt.title(f'Confusion Matrix - {model_name}')\n        plt.ylabel('True Label')\n        plt.xlabel('Predicted Label')\n        plt.xticks(rotation=45)\n        plt.yticks(rotation=0)\n        \n        if save_path:\n            plt.savefig(save_path, bbox_inches='tight')\n            self.logger.info(f\"Confusion matrix saved to {save_path}\")\n        \n        plt.show()\n    \n    def generate_training_report(self, model_name: str, save_path: str = None) -> Dict:\n        \"\"\"Generate comprehensive training report\"\"\"\n        report = {\n            'model_name': model_name,\n            'model_type': config.model.model_type,\n            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n            'configuration': {\n                'epochs': config.model.epochs,\n                'batch_size': config.model.batch_size,\n                'learning_rate': config.model.learning_rate,\n                'input_shape': config.model.input_shape\n            }\n        }\n        \n        # Add training history if available\n        if model_name in self.training_history:\n            report['training'] = self.training_history[model_name]\n        \n        # Add evaluation results if available\n        if model_name in self.evaluation_results:\n            report['evaluation'] = {\n                'accuracy': self.evaluation_results[model_name]['accuracy'],\n                'classification_report': self.evaluation_results[model_name]['classification_report']\n            }\n        \n        # Save report\n        if save_path:\n            with open(save_path, 'w') as f:\n                json.dump(report, f, indent=2, default=str)\n            self.logger.info(f\"Training report saved to {save_path}\")\n        \n        return report\n\nclass ModelValidator:\n    \"\"\"Validate model performance against requirements\"\"\"\n    \n    def __init__(self):\n        self.target_accuracy = 0.85  # 85% target accuracy\n        self.target_latency = 0.1    # 100ms target latency\n    \n    def validate_accuracy(self, accuracy: float) -> bool:\n        \"\"\"Check if accuracy meets requirements\"\"\"\n        return accuracy >= self.target_accuracy\n    \n    def validate_latency(self, model, sample_input: np.ndarray) -> Tuple[bool, float]:\n        \"\"\"Measure and validate model inference latency\"\"\"\n        # Warm up\n        for _ in range(5):\n            _ = model.predict(sample_input[:1])\n        \n        # Measure latency\n        start_time = time.time()\n        for _ in range(100):\n            _ = model.predict(sample_input[:1])\n        avg_latency = (time.time() - start_time) / 100\n        \n        meets_requirement = avg_latency <= self.target_latency\n        return meets_requirement, avg_latency\n    \n    def comprehensive_validation(self, model, X_test: np.ndarray, y_test: np.ndarray) -> Dict:\n        \"\"\"Perform comprehensive model validation\"\"\"\n        results = {}\n        \n        # Accuracy validation\n        y_pred = model.predict(X_test)\n        accuracy = accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1))\n        results['accuracy'] = {\n            'value': accuracy,\n            'meets_requirement': self.validate_accuracy(accuracy),\n            'target': self.target_accuracy\n        }\n        \n        # Latency validation\n        meets_latency, avg_latency = self.validate_latency(model, X_test)\n        results['latency'] = {\n            'value': avg_latency,\n            'meets_requirement': meets_latency,\n            'target': self.target_latency\n        }\n        \n        # Overall validation\n        results['overall_pass'] = (\n            results['accuracy']['meets_requirement'] and \n            results['latency']['meets_requirement']\n        )\n        \n        return results